# Configurações do Projeto - LLM Architecture Comparison MVP
# Copie para .env e configure conforme necessário

# === API KEYS (Opcional para MVP) ===
# OpenAI API para baseline de comparação
OPENAI_API_KEY=sk-your-openai-key-here

# Anthropic Claude (futuro)
ANTHROPIC_API_KEY=sk-ant-your-anthropic-key-here

# === CONFIGURAÇÕES DE ENVIRONMENT ===
# Force CPU se GPU der problemas
# CUDA_VISIBLE_DEVICES=-1

# PyTorch MPS fallback para M1 Mac
PYTORCH_ENABLE_MPS_FALLBACK=1

# Disable tokenizers parallelization warnings
TOKENIZERS_PARALLELISM=false

# === CACHE E STORAGE ===
# Diretório para cache dos modelos
TRANSFORMERS_CACHE=./cache
HF_HOME=./cache

# === CONFIGURAÇÕES DE PERFORMANCE ===
# Limite de memória para embeddings (MB)
MAX_MEMORY_MB=4096

# Batch size para processamento
EMBEDDING_BATCH_SIZE=32

# === CONFIGURAÇÕES DE LOGGING ===
# Nível de log (DEBUG, INFO, WARNING, ERROR)
LOG_LEVEL=INFO

# Arquivo de log principal
LOG_FILE=logs/mvp.log

# === CONFIGURAÇÕES ESPECÍFICAS MVP ===
# Número máximo de documentos para teste
MAX_TEST_DOCUMENTS=10

# Timeout para requests de API (segundos)
API_TIMEOUT=30

# Número de tentativas para APIs
API_RETRIES=3

# === CONFIGURAÇÕES DE DESENVOLVIMENTO ===
# Modo debug (true/false)
DEBUG_MODE=false

# Salvar resultados intermediários
SAVE_INTERMEDIATE_RESULTS=true

# Usar dados de cache quando disponível
USE_CACHE=true

# === STREAMLIT CONFIGURATION ===
# Porta para dashboard
STREAMLIT_PORT=8501

# Tema do dashboard (light/dark)
STREAMLIT_THEME=light

# === LIMITES E THRESHOLDS ===
# Tempo máximo de resposta aceitável (segundos)
MAX_RESPONSE_TIME=5.0

# Score mínimo de relevância
MIN_RELEVANCE_SCORE=0.1

# Custo máximo por query (USD)
MAX_COST_PER_QUERY=0.10
