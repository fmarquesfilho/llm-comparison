# src/api_baseline.py
import os
import time
import json
import logging
import random
from typing import Dict, List, Any, Optional
from pathlib import Path

logger = logging.getLogger(__name__)

class SmartMockAPI:
    """Mock inteligente que simula APIs reais de forma mais realista"""
    
    def __init__(self, provider_name: str = "openai_mock"):
        self.provider_name = provider_name
        self.knowledge_base = self._build_knowledge_base()
        
    def _build_knowledge_base(self) -> Dict[str, str]:
        """Constr√≥i base de conhecimento para respostas mais inteligentes"""
        return {
            "ru√≠do": "Conforme a NBR 10151, os limites de ru√≠do s√£o 70 dB durante o dia (7h √†s 22h) e 60 dB durante a noite (22h √†s 7h) em √°reas residenciais. A medi√ß√£o deve ser feita com medidores calibrados no limite da propriedade mais pr√≥xima aos receptores sens√≠veis.",
            
            "epi": "Os EPIs obrigat√≥rios em canteiros incluem: capacete de seguran√ßa classe A ou B, √≥culos de prote√ß√£o contra impactos, luvas adequadas √† atividade, cal√ßados de seguran√ßa com biqueira de a√ßo, e cintos de seguran√ßa para trabalhos em altura superior a 2 metros. A empresa deve fornecer gratuitamente e treinar os trabalhadores.",
            
            "concreto": "O controle de qualidade do concreto estrutural requer ensaios de resist√™ncia √† compress√£o a cada 50m¬≥ ou a cada dia de concretagem, prevalecendo o menor valor. Devem ser realizadas inspe√ß√µes regulares dos materiais e documenta√ß√£o completa de todos os procedimentos.",
            
            "obras noturnas": "Obras noturnas necessitam autoriza√ß√£o especial da prefeitura, ilumina√ß√£o m√≠nima de 200 lux, sinaliza√ß√£o refor√ßada com dispositivos refletivos e luminosos, e equipes especializadas. O hor√°rio permitido geralmente √© das 22h √†s 6h, dependendo da √°rea urbana.",
            
            "eia": "O Estudo de Impacto Ambiental (EIA/RIMA) √© obrigat√≥rio para obras com √°rea superior a 3000m¬≤. Tamb√©m √© necess√°rio licenciamento pr√©vio, plano de gerenciamento de res√≠duos s√≥lidos e medidas de controle de eros√£o e prote√ß√£o de recursos h√≠dricos.",
            
            "qualidade": "O controle de qualidade em constru√ß√£o envolve inspe√ß√µes regulares dos materiais, ensaios de resist√™ncia, documenta√ß√£o completa e auditorias internas peri√≥dicas. Para concreto estrutural, os ensaios devem ser realizados conforme cronograma espec√≠fico baseado no volume ou tempo de concretagem."
        }
    
    def _find_best_match(self, question: str) -> str:
        """Encontra a melhor resposta baseada na pergunta"""
        question_lower = question.lower()
        
        # Procura palavras-chave na pergunta
        for keyword, answer in self.knowledge_base.items():
            if keyword in question_lower:
                return answer
        
        # Se n√£o encontrar match espec√≠fico, resposta gen√©rica
        return "Com base nos documentos fornecidos, posso ajudar com informa√ß√µes sobre normas de constru√ß√£o civil, EPIs, controle de qualidade, impacto ambiental e regulamenta√ß√µes. Poderia reformular sua pergunta de forma mais espec√≠fica?"
    
    def _simulate_response_delay(self) -> float:
        """Simula delay realista de API"""
        # Simula lat√™ncia de rede + processamento
        base_delay = random.uniform(0.5, 1.5)  # Lat√™ncia base
        processing_delay = random.uniform(0.8, 2.2)  # Processamento LLM
        return base_delay + processing_delay
    
    def _estimate_tokens(self, question: str, answer: str, context: str = "") -> int:
        """Estima tokens de forma mais realista"""
        # Estimativa aproximada: ~4 chars por token
        prompt_tokens = len(f"Sistema: Voc√™ √© um assistente...\nContexto: {context}\nPergunta: {question}") // 4
        completion_tokens = len(answer) // 4
        return prompt_tokens + completion_tokens

class APIBaseline:
    """Cliente para APIs externas com mock inteligente quando APIs n√£o dispon√≠veis"""
    
    def __init__(self, api_provider: str = "openai"):
        self.api_provider = api_provider
        self.client = None
        self.mock_api = SmartMockAPI(f"{api_provider}_mock")
        self.use_mock = False
        
        self.cost_per_1k_tokens = {
            "openai": 0.002,  # GPT-3.5-turbo
            "anthropic": 0.003,  # Claude Haiku
            "openai_mock": 0.002,  # Mesmo custo para compara√ß√£o
            "anthropic_mock": 0.003
        }
        
        self._setup_client()
    
    def _setup_client(self):
        """Configura cliente da API real ou mock"""
        try:
            if self.api_provider == "openai":
                api_key = os.getenv("OPENAI_API_KEY")
                if not api_key:
                    logger.warning("‚ö†Ô∏è OPENAI_API_KEY n√£o encontrada - usando mock inteligente")
                    self.use_mock = True
                    return True
                
                import openai
                self.client = openai.OpenAI(api_key=api_key)
                
                # Testa conex√£o com request simples
                try:
                    response = self.client.chat.completions.create(
                        model="gpt-3.5-turbo",
                        messages=[{"role": "user", "content": "test"}],
                        max_tokens=1
                    )
                    logger.info("‚úÖ Cliente OpenAI configurado e testado")
                    return True
                    
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Erro ao testar OpenAI (quota/limite?): {e}")
                    logger.info("üîÑ Usando mock inteligente")
                    self.use_mock = True
                    return True
                
            elif self.api_provider == "anthropic":
                api_key = os.getenv("ANTHROPIC_API_KEY")
                if not api_key:
                    logger.warning("‚ö†Ô∏è ANTHROPIC_API_KEY n√£o encontrada - usando mock")
                    self.use_mock = True
                    return True
                
                # Para o MVP, sempre usar mock para Anthropic
                logger.info("üîÑ Usando mock Anthropic (implementar cliente real se necess√°rio)")
                self.use_mock = True
                return True
                
            return True
            
        except ImportError as e:
            logger.warning(f"‚ö†Ô∏è Biblioteca da API n√£o instalada: {e} - usando mock")
            self.use_mock = True
            return True
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erro na configura√ß√£o API: {e} - usando mock")
            self.use_mock = True
            return True
    
    def create_context_from_docs(self, documents: List[Dict], top_k: int = 3) -> str:
        """Cria contexto a partir dos documentos mais relevantes"""
        if not documents:
            return ""
        
        context_docs = documents[:top_k]
        
        context = "Documentos relevantes:\n\n"
        for i, doc in enumerate(context_docs, 1):
            content = doc.get('content', str(doc))
            # Limita tamanho para n√£o estourar limite de tokens
            if len(content) > 400:
                content = content[:400] + "..."
            context += f"Documento {i}: {doc.get('title', 'Sem t√≠tulo')}\n{content}\n\n"
        
        return context
    
    def query_openai(self, question: str, context: str = "") -> Dict[str, Any]:
        """Consulta OpenAI GPT (real ou mock)"""
        
        if self.use_mock:
            return self._query_mock(question, context, "openai")
        
        if not self.client:
            return {
                "answer": "‚ùå Cliente OpenAI n√£o configurado",
                "error": "API key n√£o encontrada"
            }
        
        try:
            system_prompt = "Voc√™ √© um assistente especializado em constru√ß√£o civil e engenharia. Responda com base apenas nas informa√ß√µes fornecidas nos documentos."
            
            prompt = f"""Com base nos documentos fornecidos, responda a pergunta de forma clara e precisa.

{context}

Pergunta: {question}

Resposta:"""
            
            start_time = time.time()
            
            response = self.client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=300,
                temperature=0.7
            )
            
            response_time = time.time() - start_time
            
            answer = response.choices[0].message.content.strip()
            
            # Estima custo baseado em tokens
            total_tokens = response.usage.total_tokens
            estimated_cost = (total_tokens / 1000) * self.cost_per_1k_tokens["openai"]
            
            return {
                "answer": answer,
                "response_time": response_time,
                "tokens_used": total_tokens,
                "estimated_cost_usd": estimated_cost,
                "model": "gpt-3.5-turbo",
                "source": "openai_api"
            }
            
        except Exception as e:
            logger.error(f"‚ùå Erro na consulta OpenAI: {e}")
            logger.info("üîÑ Fallback para mock")
            return self._query_mock(question, context, "openai")
    
    def _query_mock(self, question: str, context: str, provider: str) -> Dict[str, Any]:
        """Query usando mock inteligente"""
        start_time = time.time()
        
        # Simula delay realista
        delay = self.mock_api._simulate_response_delay()
        time.sleep(delay)
        
        # Gera resposta inteligente
        answer = self.mock_api._find_best_match(question)
        
        # Se h√° contexto, tenta usar informa√ß√µes dos documentos
        if context and len(context) > 50:
            # Extrai algumas informa√ß√µes do contexto
            context_keywords = ["NBR", "dB", "EPIs", "capacete", "50m¬≥", "200 lux", "3000m¬≤"]
            found_keywords = [kw for kw in context_keywords if kw in context]
            
            if found_keywords:
                answer = f"Baseado nos documentos fornecidos: {answer}"
                if len(found_keywords) >= 2:
                    answer += f" (Refer√™ncias encontradas: {', '.join(found_keywords[:3])})"
        
        response_time = time.time() - start_time
        
        # Estima tokens e custo
        estimated_tokens = self.mock_api._estimate_tokens(question, answer, context)
        estimated_cost = (estimated_tokens / 1000) * self.cost_per_1k_tokens.get(f"{provider}_mock", 0.002)
        
        return {
            "answer": answer,
            "response_time": response_time,
            "tokens_used": estimated_tokens,
            "estimated_cost_usd": estimated_cost,
            "model": f"{provider}-mock",
            "source": f"{provider}_mock",
            "mock_used": True
        }
    
    def query_anthropic_mock(self, question: str, context: str = "") -> Dict[str, Any]:
        """Mock do Anthropic Claude mais realista"""
        return self._query_mock(question, context, "anthropic")
    
    def query(self, question: str, documents: List[Dict] = None) -> Dict[str, Any]:
        """Interface principal para consultas √† API"""
        
        # Cria contexto se documentos fornecidos
        context = ""
        if documents:
            context = self.create_context_from_docs(documents)
        
        # Chama API apropriada
        if self.api_provider == "openai":
            result = self.query_openai(question, context)
        elif self.api_provider == "anthropic":
            result = self.query_anthropic_mock(question, context)
        else:
            result = {
                "answer": f"‚ùå Provedor '{self.api_provider}' n√£o suportado",
                "error": "Provedor inv√°lido"
            }
        
        # Adiciona metadados
        result.update({
            "question": question,
            "api_provider": self.api_provider,
            "context_provided": len(context) > 0,
            "num_context_docs": len(documents) if documents else 0
        })
        
        return result
    
    def batch_query(self, questions: List[str], documents: List[Dict] = None) -> List[Dict[str, Any]]:
        """Processa m√∫ltiplas perguntas"""
        results = []
        
        for question in questions:
            result = self.query(question, documents)
            results.append(result)
            
            # Pequena pausa para simular rate limit realista
            if not self.use_mock:
                time.sleep(0.2)
            else:
                time.sleep(0.05)  # Mock mais r√°pido
        
        return results
    
    def get_cost_estimate(self, num_queries: int, avg_tokens_per_query: int = 200) -> Dict[str, float]:
        """Estima custos para um volume de consultas"""
        
        provider_key = f"{self.api_provider}_mock" if self.use_mock else self.api_provider
        cost_per_1k = self.cost_per_1k_tokens.get(provider_key, 0.002)
        
        total_tokens = num_queries * avg_tokens_per_query
        total_cost = (total_tokens / 1000) * cost_per_1k
        
        return {
            "num_queries": num_queries,
            "avg_tokens_per_query": avg_tokens_per_query,
            "total_tokens": total_tokens,
            "cost_per_1k_tokens": cost_per_1k,
            "total_cost_usd": total_cost,
            "cost_per_query_usd": total_cost / num_queries if num_queries > 0 else 0,
            "using_mock": self.use_mock,
            "provider": f"{self.api_provider}_mock" if self.use_mock else self.api_provider
        }
    
    def get_system_info(self) -> Dict[str, Any]:
        """Retorna informa√ß√µes do sistema"""
        return {
            "api_provider": self.api_provider,
            "using_mock": self.use_mock,
            "client_configured": self.client is not None,
            "cost_per_1k_tokens": self.cost_per_1k_tokens.get(
                f"{self.api_provider}_mock" if self.use_mock else self.api_provider, 0.002
            ),
            "mock_knowledge_topics": list(self.mock_api.knowledge_base.keys()) if self.use_mock else []
        }


def test_api_baseline():
    """Fun√ß√£o de teste para baseline API"""
    logger.info("üß™ Testando baseline API...")
    
    # Carrega documentos de teste
    data_dir = Path("data/raw")
    documents = []
    
    if data_dir.exists():
        for json_file in data_dir.glob("*.json"):
            with open(json_file, 'r', encoding='utf-8') as f:
                doc = json.load(f)
                documents.append(doc)
    
    # Carrega perguntas de teste
    questions_file = Path("data/evaluation/test_questions.json")
    if not questions_file.exists():
        logger.error("‚ùå Perguntas de teste n√£o encontradas")
        return []
    
    with open(questions_file, 'r', encoding='utf-8') as f:
        test_questions = json.load(f)
    
    # Testa OpenAI (real ou mock)
    openai_baseline = APIBaseline("openai")
    
    # Log info do sistema
    system_info = openai_baseline.get_system_info()
    logger.info(f"Sistema API: {system_info}")
    
    results = []
    
    for q in test_questions:
        result = openai_baseline.query(q['question'], documents)
        
        # Adiciona informa√ß√µes da pergunta
        result.update({
            'question_id': q['id'],
            'expected_concepts': q['expected_concepts'],
            'category': q['category']
        })
        
        results.append(result)
        
        source = "MOCK" if result.get('mock_used', False) else "API"
        logger.info(f"API {q['id']} [{source}]: {result.get('response_time', 0):.2f}s, "
                   f"custo: ${result.get('estimated_cost_usd', 0):.4f}")
    
    # Salva resultados
    results_file = Path("data/evaluation/api_baseline_results.json")
    results_file.parent.mkdir(parents=True, exist_ok=True)
    
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    # Log resumo
    total_cost = sum(r.get('estimated_cost_usd', 0) for r in results)
    avg_time = sum(r.get('response_time', 0) for r in results) / len(results)
    mock_count = sum(1 for r in results if r.get('mock_used', False))
    
    logger.info(f"‚úÖ Baseline API testado:")
    logger.info(f"   ‚Ä¢ Resultados: {len(results)}")
    logger.info(f"   ‚Ä¢ Tempo m√©dio: {avg_time:.2f}s")
    logger.info(f"   ‚Ä¢ Custo total: ${total_cost:.4f}")
    logger.info(f"   ‚Ä¢ Mocks usados: {mock_count}/{len(results)}")
    logger.info(f"   ‚Ä¢ Arquivo: {results_file}")
    
    return results


if __name__ == "__main__":
    # Teste standalone
    logging.basicConfig(level=logging.INFO)
    test_api_baseline()
  