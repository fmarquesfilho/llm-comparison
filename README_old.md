# ğŸ¤– LLM Architecture Comparison - MVP

Projeto simplificado para comparaÃ§Ã£o rÃ¡pida entre arquiteturas RAG local vs APIs externas, focado em viabilidade tÃ©cnica e econÃ´mica para decisÃ£o executiva.

## ğŸ¯ Objetivo do MVP

Validar em **2 semanas** (40h) qual arquitetura Ã© mais adequada para o domÃ­nio de construÃ§Ã£o civil:
- **RAG Local**: Embeddings + FAISS + recuperaÃ§Ã£o simples
- **API Externa**: OpenAI/Claude com contexto (com mock inteligente)

## ğŸš€ Quick Start

### 1. Setup Ambiente (Mac M1 otimizado)

```bash
# Clone o repositÃ³rio
git clone <repo>
cd otoh-llm-comparison

# Crie ambiente conda
conda env create -f environment.yml
conda activate llm-comparison

# Atualiza ambiente
conda env update -f environment.yml

# Configure API keys (OPCIONAL - sistema funciona sem)
echo "OPENAI_API_KEY=sua_key_aqui" > .env
```

### 2. Teste RÃ¡pido do Sistema

```bash
# Valide ambiente rapidamente
python quick_test.py
```

### 3. ExecuÃ§Ã£o Completa do MVP

```bash
# Execute o pipeline completo
python run_mvp.py
```

Este comando irÃ¡:
1. âœ… Verificar ambiente
2. ğŸ“ Criar dados sintÃ©ticos de exemplo (5 documentos)
3. ğŸ¤– Configurar RAG com embeddings
4. ğŸ§ª Testar RAG com 5 perguntas
5. ğŸŒ Testar API externa (real ou mock inteligente)
6. ğŸ’° Calcular estimativas de custo
7. ğŸ“Š Gerar relatÃ³rio de comparaÃ§Ã£o

### 4. Visualizar Resultados

```bash
# LanÃ§ar dashboard interativo
streamlit run app/dashboard.py
```

Acesse: http://localhost:8501

## ğŸ“ Estrutura do Projeto

```
otoh-llm-comparison/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                 # Documentos JSON (criados automaticamente)
â”‚   â”œâ”€â”€ embeddings/          # Ãndice FAISS + metadados
â”‚   â””â”€â”€ evaluation/          # Resultados e relatÃ³rios
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py           # ConfiguraÃ§Ãµes centralizadas
â”‚   â”œâ”€â”€ simple_rag.py       # RAG simplificado (sÃ³ embeddings)
â”‚   â”œâ”€â”€ api_baseline.py     # Cliente APIs externas + mock inteligente
â”‚   â””â”€â”€ evaluator.py        # MÃ©tricas e comparaÃ§Ãµes
â”œâ”€â”€ app/
â”‚   â””â”€â”€ dashboard.py        # Interface Streamlit
â”œâ”€â”€ requirements.txt        # DependÃªncias mÃ­nimas
â”œâ”€â”€ run_mvp.py             # Script principal
â””â”€â”€ quick_test.py          # ValidaÃ§Ã£o rÃ¡pida do ambiente
```

## ğŸ›ï¸ ConfiguraÃ§Ã£o

### Modelos Otimizados para MVP

- **Embedding**: `all-MiniLM-L6-v2` (rÃ¡pido, 22MB)
- **GeraÃ§Ã£o**: Apenas retrieval simples (sem LLM local)
- **API Externa**: GPT-3.5-turbo (real) ou mock inteligente

### Sistema de Fallback Inteligente

O sistema **SEMPRE funciona**, mesmo sem API keys:

```bash
# CenÃ¡rio 1: Com OpenAI API key vÃ¡lida
OPENAI_API_KEY=sk-... python run_mvp.py
# âœ… Usa API real da OpenAI

# CenÃ¡rio 2: Sem API key ou quota esgotada
python run_mvp.py  
# âœ… Usa mock inteligente com respostas realistas

# CenÃ¡rio 3: Problema de conexÃ£o
# âœ… Fallback automÃ¡tico para mock
```

### Dados de Exemplo

O sistema cria automaticamente 5 documentos sobre:
- Normas de ruÃ­do (NBR 10151)
- EPIs obrigatÃ³rios
- Controle ambiental
- Obras noturnas
- Controle de qualidade

### Perguntas de Teste

5 perguntas cobrindo diferentes categorias:
1. Limites de ruÃ­do permitidos
2. Equipamentos de proteÃ§Ã£o obrigatÃ³rios
3. Controle de qualidade do concreto
4. Requisitos para obras noturnas
5. Quando fazer EIA/RIMA

## ğŸ“Š MÃ©tricas Avaliadas

### Quantitativas
- **Tempo de resposta** (segundos)
- **Custo por query** (USD)
- **Taxa de documentos relevantes** (%)
- **Cobertura de conceitos esperados** (%)

### Qualitativas (Manual)
- **PrecisÃ£o da resposta** (1-5)
- **Completude da informaÃ§Ã£o** (1-5)
- **Usabilidade geral** (1-5)

### CritÃ©rios de DecisÃ£o
- âœ… **< $0.10 por query**: ViÃ¡vel economicamente
- âœ… **< 3 segundos**: ExperiÃªncia aceitÃ¡vel  
- âœ… **> 70% relevÃ¢ncia**: Qualidade mÃ­nima
- âœ… **> 3.0/5.0 qualidade**: AprovaÃ§Ã£o usuÃ¡rios

## ğŸ’° AnÃ¡lise de Custos

### CenÃ¡rios Testados
- **Piloto**: 100 queries/dia
- **ProduÃ§Ã£o Pequena**: 1K queries/dia
- **ProduÃ§Ã£o MÃ©dia**: 5K queries/dia

### ComparaÃ§Ã£o Estimada
| Arquitetura | Setup | Custo/Query | Custo/MÃªs (1K/dia) |
|-------------|--------|-------------|---------------------|
| RAG Local   | $0     | ~$0.000     | ~$0                |
| OpenAI API  | $0     | ~$0.002     | ~$60               |
| Mock API    | $0     | ~$0.002*    | ~$60*              |

*Custo estimativo para comparaÃ§Ã£o - mock Ã© gratuito

## ğŸ”§ Desenvolvimento

### Executar Componentes Individuais

```bash
# Apenas RAG
python src/simple_rag.py

# Apenas API baseline (com fallback automÃ¡tico)
python src/api_baseline.py

# Apenas avaliaÃ§Ã£o
python src/evaluator.py

# Validar configuraÃ§Ã£o
python -c "from src.config import Config; Config.validate_setup()"
```

### Estrutura de Dados

**Documento JSON:**
```json
{
  "id": "doc_001",
  "title": "Normas de RuÃ­do",
  "content": "O monitoramento de ruÃ­do...",
  "category": "regulamentacao",
  "keywords": ["ruÃ­do", "NBR 10151"]
}
```

**Resultado de Query:**
```json
{
  "question": "Quais os limites de ruÃ­do?",
  "answer": "Baseado no documento...",
  "response_time": 1.23,
  "relevance_score": 0.85,
  "retrieved_docs": 3,
  "mock_used": false
}
```

## ğŸ“ˆ Dashboard

### Abas DisponÃ­veis
1. **ğŸ“‹ Resumo Executivo**: RecomendaÃ§Ã£o + prÃ³ximos passos
2. **âš¡ Performance**: Tempo de resposta + taxa de sucesso
3. **ğŸ’° Custos**: ComparaÃ§Ã£o por cenÃ¡rio (real vs estimado)
4. **ğŸ¯ Qualidade**: AnÃ¡lise de relevÃ¢ncia + conceitos
5. **ğŸ” Detalhes**: Resultados completos por pergunta

### Funcionalidades
- ComparaÃ§Ã£o side-by-side RAG vs API
- IndicaÃ§Ã£o clara quando mock Ã© usado
- Filtros por categoria/qualidade
- MÃ©tricas em tempo real
- GrÃ¡ficos interativos com Plotly

## ğŸš¦ Troubleshooting

### Problemas Comuns

**1. Erro no PyTorch/MPS:**
```bash
# Force CPU se MPS der problema
export PYTORCH_ENABLE_MPS_FALLBACK=1
python run_mvp.py
```

**2. Sem quota OpenAI:**
```bash
# Sistema funciona normalmente com mock
unset OPENAI_API_KEY  # Remove key invÃ¡lida
python run_mvp.py     # Usa mock automaticamente
```

**3. DependÃªncias faltando:**
```bash
# Execute teste completo primeiro
python quick_test.py

# Se falhar, reinstale
pip install --upgrade -r requirements.txt
```

**4. FAISS nÃ£o instala:**
```bash
# Com conda (melhor para M1 Mac)
conda install -c conda-forge faiss-cpu

# Ou com pip se conda falhar
pip install faiss-cpu --force-reinstall
```

**5. Conflitos de ambiente:**
```bash
# Lista ambientes conda
conda env list

# Remove ambiente se corrupto
conda env remove -n llm-comparison

# Recria ambiente limpo
conda create -n llm-comparison python=3.9 -y
```

### Logs Detalhados

```bash
# Debug completo
export LOG_LEVEL=DEBUG
python run_mvp.py
```

### ValidaÃ§Ã£o do Sistema

```bash
# Teste completo do ambiente
python quick_test.py

# ValidaÃ§Ã£o especÃ­fica do RAG
python -c "from src.simple_rag import test_simple_rag; test_simple_rag()"

# ValidaÃ§Ã£o da API (com fallback)
python -c "from src.api_baseline import test_api_baseline; test_api_baseline()"
```

## ğŸ“‹ Checklist MVP

### Funcionalidades âœ…
- [x] Setup ambiente M1 compatÃ­vel
- [x] RAG bÃ¡sico funcional com 5 documentos
- [x] Testes automÃ¡ticos com 5 perguntas
- [x] API baseline com fallback inteligente
- [x] Sistema de mock realista
- [x] MÃ©tricas essenciais
- [x] Dashboard Streamlit completo
- [x] AnÃ¡lise de custos comparativa
- [x] RelatÃ³rio executivo
- [x] DocumentaÃ§Ã£o completa

### ValidaÃ§Ã£o de Qualidade âœ…
- [x] Funciona sem API keys
- [x] Fallback automÃ¡tico para mock
- [x] Respostas realistas no mock
- [x] MÃ©tricas consistentes
- [x] Interface intuitiva

## ğŸ¯ CritÃ©rio de Sucesso

**âœ… Stakeholder consegue decidir** entre RAG local vs API externa baseado em:
- Dados objetivos de performance
- AnÃ¡lise clara de custos (real + estimado)
- RecomendaÃ§Ã£o justificada com trade-offs
- Sistema que sempre funciona (com ou sem APIs)
- Interface visual para anÃ¡lise

## ğŸ”„ PrÃ³ximas IteraÃ§Ãµes

**Se MVP validar viabilidade:**
- Expandir para mais documentos reais
- Implementar fine-tuning com LoRA
- MÃ©tricas avanÃ§adas (ROUGE, BLEU)
- Pipeline automatizado CI/CD
- APIs reais (Anthropic Claude, etc.)

**Se MVP mostrar inviabilidade:**
- Pivot para apenas APIs externas
- Foco em integraÃ§Ã£o/UX
- AnÃ¡lise detalhada de custos
- AvaliaÃ§Ã£o de outras arquiteturas

## ğŸš€ Funcionalidades Exclusivas

### Mock Inteligente
- **Respostas contextualmente relevantes** baseadas em palavras-chave
- **SimulaÃ§Ã£o realista de latÃªncia** de rede + processamento
- **Estimativas precisas de tokens** e custos
- **Fallback automÃ¡tico** quando API real falha

### Dashboard AvanÃ§ado
- **GrÃ¡ficos interativos** com Plotly
- **ComparaÃ§Ã£o visual** entre arquiteturas
- **MÃ©tricas em tempo real** com caching
- **IndicaÃ§Ã£o clara** de uso de mock vs API real

### Sistema de AvaliaÃ§Ã£o
- **MÃ©tricas objetivas** automatizadas
- **RecomendaÃ§Ãµes inteligentes** baseadas em dados
- **AnÃ¡lise de trade-offs** estruturada
- **RelatÃ³rios executivos** para tomada de decisÃ£o

## ğŸ“ Suporte

Para problemas tÃ©cnicos:
1. Execute `python quick_test.py` para diagnÃ³stico
2. Verifique logs em `logs/`
3. Execute `Config.validate_setup()` para validaÃ§Ã£o
4. Consulte troubleshooting acima

**Garantia**: O sistema sempre funciona, mesmo sem APIs externas!

---

**Tempo estimado total**: 5-15 minutos para setup + execuÃ§Ã£o completa  
**Dados necessÃ¡rios**: Nenhum (usa dados sintÃ©ticos)  
**DependÃªncias externas**: Nenhuma obrigatÃ³ria (OpenAI opcional)  
**Compatibilidade**: macOS (M1/Intel), Linux, Windows
