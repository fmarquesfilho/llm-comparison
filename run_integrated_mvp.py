#!/usr/bin/env python3
"""
MVP Integrado - Compara√ß√£o Multi-Cen√°rio RAG
Adaptado para incluir os tr√™s cen√°rios: Vector RAG, Hybrid RAG e LLM-Only

Execute com: python run_integrated_mvp.py
"""

import os
import sys
import json
import time
import logging
from pathlib import Path
from typing import Dict, List, Any
import pandas as pd

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def check_environment():
    """Verifica ambiente com depend√™ncias para os tr√™s cen√°rios"""
    logger.info("üîç Verificando ambiente para multi-cen√°rio...")
    
    missing_deps = []
    
    try:
        import torch
        import transformers
        import sentence_transformers
        import faiss
        import networkx
        import streamlit
        logger.info("‚úÖ Depend√™ncias principais encontradas")
    except ImportError as e:
        missing_deps.append(str(e))
    
    if missing_deps:
        logger.error("‚ùå Depend√™ncias faltando:")
        for dep in missing_deps:
            logger.error(f"  {dep}")
        logger.error("Execute: conda env update -f environment.yml")
        return False
    
    return True

def create_enhanced_sample_data():
    """Cria dados de exemplo enriquecidos com informa√ß√µes temporais"""
    logger.info("üìù Criando dados de exemplo com contexto temporal...")
    
    data_dir = Path("data/raw")
    data_dir.mkdir(parents=True, exist_ok=True)
    
    # Dados enriquecidos com contexto temporal e entidades
    enhanced_docs = [
        {
            "id": "doc_001",
            "title": "Normas de Ru√≠do - NBR 10151:2019",
            "content": "O monitoramento de ru√≠do em obras urbanas deve seguir as diretrizes da NBR 10151, revis√£o 2019. Os n√≠veis de ru√≠do n√£o podem exceder 70 dB durante o per√≠odo diurno (das 7h √†s 22h) e 60 dB durante o per√≠odo noturno (das 22h √†s 7h) em √°reas residenciais. √â obrigat√≥rio o uso de medidores calibrados classe 1 ou 2, com certificado de calibra√ß√£o v√°lido. A medi√ß√£o deve ser feita no limite da propriedade mais pr√≥xima aos receptores sens√≠veis. Para obras que ultrapassem 60 dias consecutivos, √© necess√°rio relat√≥rio mensal de monitoramento. Em caso de reclama√ß√µes, medi√ß√µes extraordin√°rias devem ser realizadas em at√© 48 horas.",
            "category": "regulamentacao",
            "keywords": ["ru√≠do", "NBR 10151", "medi√ß√£o", "limites", "obras urbanas", "hor√°rio", "calibra√ß√£o"],
            "temporal_markers": ["7h √†s 22h", "22h √†s 7h", "60 dias", "mensal", "48 horas"],
            "entities": ["NBR 10151", "70 dB", "60 dB", "medidores classe 1", "certificado calibra√ß√£o"]
        },
        {
            "id": "doc_002", 
            "title": "EPIs Obrigat√≥rios - NR 6:2020",
            "content": "Equipamentos de prote√ß√£o individual (EPIs) obrigat√≥rios em canteiros incluem: capacete de seguran√ßa classe A ou B, obrigat√≥rio durante todo o per√≠odo de trabalho das 6h √†s 18h. √ìculos de prote√ß√£o contra impactos para atividades com risco de proje√ß√£o de part√≠culas. Luvas adequadas ao tipo de atividade (n√£o usar pr√≥ximo a m√°quinas rotativas). Cal√ßados de seguran√ßa com biqueira de a√ßo, obrigat√≥rios em toda a √°rea da obra. Cintos de seguran√ßa para trabalho em altura acima de 2 metros, com inspe√ß√£o di√°ria obrigat√≥ria. A empresa deve fornecer gratuitamente, treinar os trabalhadores sobre o uso correto no primeiro dia de trabalho e substituir quando necess√°rio. Inspe√ß√£o semanal dos EPIs √© obrigat√≥ria, com registro em planilha. EPIs danificados devem ser substitu√≠dos imediatamente.",
            "category": "seguranca",
            "keywords": ["EPIs", "capacete", "√≥culos", "luvas", "cal√ßados", "cintos", "treinamento", "inspe√ß√£o"],
            "temporal_markers": ["6h √†s 18h", "primeiro dia", "di√°ria", "semanal", "imediatamente"],
            "entities": ["NR 6", "classe A", "classe B", "2 metros", "biqueira de a√ßo"]
        },
        {
            "id": "doc_003",
            "title": "Controle de Qualidade do Concreto - NBR 12655:2015",
            "content": "O controle de qualidade do concreto estrutural requer ensaios de resist√™ncia √† compress√£o a cada 50m¬≥ ou a cada dia de concretagem, prevalecendo o menor valor. Os corpos de prova devem ser moldados no local da obra, curados em condi√ß√µes padronizadas por 28 dias para ensaio definitivo. Ensaios intermedi√°rios aos 7 dias e 14 dias permitem avalia√ß√£o pr√©via da evolu√ß√£o da resist√™ncia. Para obras de grande porte (acima de 500m¬≥), ensaios adicionais de m√≥dulo de elasticidade s√£o recomendados mensalmente. Documenta√ß√£o completa deve ser mantida por no m√≠nimo 5 anos ap√≥s conclus√£o da obra. Em caso de resultado insatisfat√≥rio, nova amostragem deve ser feita imediatamente e ensaios refeitos em 48 horas.",
            "category": "qualidade",
            "keywords": ["controle qualidade", "concreto", "ensaios", "50m¬≥", "28 dias", "resist√™ncia", "documenta√ß√£o"],
            "temporal_markers": ["50m¬≥", "dia de concretagem", "28 dias", "7 dias", "14 dias", "mensalmente", "5 anos", "48 horas"],
            "entities": ["NBR 12655", "28 dias", "500m¬≥", "m√≥dulo elasticidade"]
        },
        {
            "id": "doc_004",
            "title": "Obras Noturnas - Lei Municipal 16.402/2016",
            "content": "Obras noturnas requerem autoriza√ß√£o especial da prefeitura municipal, com solicita√ß√£o feita com anteced√™ncia m√≠nima de 15 dias √∫teis. Ilumina√ß√£o adequada com intensidade m√≠nima de 200 lux deve ser mantida em toda a √°rea de trabalho durante o per√≠odo noturno (das 22h √†s 6h). Sinaliza√ß√£o refor√ßada com dispositivos refletivos e luminosos deve ser instalada at√© 50 metros antes da obra. Equipes especializadas com treinamento espec√≠fico em seguran√ßa noturna s√£o obrigat√≥rias, com certifica√ß√£o renovada anualmente. O hor√°rio permitido varia entre 22h e 6h em √°reas comerciais, e entre 23h e 5h em √°reas residenciais. Relat√≥rio di√°rio de atividades deve ser enviado √† fiscaliza√ß√£o at√© √†s 8h do dia seguinte.",
            "category": "regulamentacao", 
            "keywords": ["obras noturnas", "autoriza√ß√£o", "ilumina√ß√£o", "200 lux", "sinaliza√ß√£o", "22h-6h"],
            "temporal_markers": ["15 dias √∫teis", "22h √†s 6h", "23h e 5h", "anualmente", "8h do dia seguinte"],
            "entities": ["Lei 16.402/2016", "200 lux", "50 metros", "22h-6h", "23h-5h"]
        },
        {
            "id": "doc_005",
            "title": "Controle Ambiental - Resolu√ß√£o CONAMA 307/2002",
            "content": "O controle de impacto ambiental em constru√ß√µes requer licenciamento pr√©vio do √≥rg√£o competente, solicitado com anteced√™ncia m√≠nima de 90 dias do in√≠cio da obra. Elabora√ß√£o de plano de gerenciamento de res√≠duos s√≥lidos √© obrigat√≥ria, com revis√£o trimestral. Implementa√ß√£o de medidas de controle de eros√£o deve ocorrer antes do per√≠odo chuvoso. Prote√ß√£o de recursos h√≠dricos com barreiras f√≠sicas num raio de 30 metros de corpos d'√°gua. Obras com √°rea superior a 3000m¬≤ necessitam de estudo de impacto ambiental (EIA/RIMA), com prazo de an√°lise de 180 dias. Monitoramento da qualidade do ar deve ser realizado semanalmente durante toda a obra. Relat√≥rios mensais devem ser enviados aos √≥rg√£os ambientais at√© o 5¬∫ dia √∫til do m√™s seguinte.",
            "category": "meio_ambiente",
            "keywords": ["licenciamento", "res√≠duos", "eros√£o", "recursos h√≠dricos", "EIA", "RIMA"],
            "temporal_markers": ["90 dias", "trimestral", "per√≠odo chuvoso", "180 dias", "semanalmente", "mensalmente", "5¬∫ dia √∫til"],
            "entities": ["CONAMA 307/2002", "3000m¬≤", "30 metros", "EIA/RIMA"]
        }
    ]
    
    # Salva documentos
    for doc in enhanced_docs:
        doc_file = data_dir / f"{doc['id']}.json"
        with open(doc_file, 'w', encoding='utf-8') as f:
            json.dump(doc, f, ensure_ascii=False, indent=2)
    
    logger.info(f"‚úÖ Criados {len(enhanced_docs)} documentos enriquecidos em {data_dir}")
    return enhanced_docs

def create_temporal_test_questions():
    """Cria perguntas de teste com caracter√≠sticas temporais espec√≠ficas"""
    temporal_questions = [
        {
            "id": "q001",
            "question": "Quais s√£o os limites de ru√≠do permitidos durante o dia e durante a noite?",
            "expected_concepts": ["70 dB", "60 dB", "7h √†s 22h", "22h √†s 7h", "NBR 10151"],
            "category": "regulamentacao",
            "temporal_focus": "schedule",
            "complexity": "basic"
        },
        {
            "id": "q002", 
            "question": "Com que frequ√™ncia devem ser realizados os ensaios de resist√™ncia do concreto?",
            "expected_concepts": ["50m¬≥", "dia de concretagem", "28 dias", "ensaios"],
            "category": "qualidade",
            "temporal_focus": "frequency",
            "complexity": "intermediate"
        },
        {
            "id": "q003",
            "question": "Que procedimentos temporais s√£o necess√°rios antes de iniciar obras noturnas?",
            "expected_concepts": ["15 dias √∫teis", "autoriza√ß√£o", "anteced√™ncia", "prefeitura"],
            "category": "regulamentacao",
            "temporal_focus": "sequence",
            "complexity": "intermediate"
        },
        {
            "id": "q004",
            "question": "Qual o prazo de an√°lise para EIA/RIMA e quando deve ser solicitado?",
            "expected_concepts": ["180 dias", "90 dias", "3000m¬≤", "EIA/RIMA"],
            "category": "meio_ambiente",
            "temporal_focus": "duration",
            "complexity": "advanced"
        },
        {
            "id": "q005",
            "question": "Quando os EPIs devem ser inspecionados e por quanto tempo manter documenta√ß√£o?",
            "expected_concepts": ["semanal", "di√°ria", "5 anos", "inspe√ß√£o"],
            "category": "seguranca",
            "temporal_focus": "multiple_timeframes",
            "complexity": "advanced"
        }
    ]
    
    # Salva perguntas
    questions_file = Path("data/evaluation/temporal_test_questions.json")
    questions_file.parent.mkdir(parents=True, exist_ok=True)
    
    with open(questions_file, 'w', encoding='utf-8') as f:
        json.dump(temporal_questions, f, ensure_ascii=False, indent=2)
    
    logger.info(f"‚úÖ Criadas {len(temporal_questions)} perguntas temporais")
    return temporal_questions

def run_multi_scenario_comparison():
    """Executa compara√ß√£o completa entre os tr√™s cen√°rios"""
    logger.info("üöÄ Iniciando compara√ß√£o multi-cen√°rio...")
    
    try:
        # Importa sistema multi-cen√°rio
        sys.path.append('src')
        from multi_scenario_system import MultiScenarioSystem
        
        # Carrega dados
        data_dir = Path("data/raw")
        documents = []
        
        for json_file in data_dir.glob("*.json"):
            with open(json_file, 'r', encoding='utf-8') as f:
                doc = json.load(f)
                documents.append(doc)
        
        if not documents:
            logger.error("‚ùå Nenhum documento encontrado")
            return []
        
        # Carrega perguntas
        questions_file = Path("data/evaluation/temporal_test_questions.json")
        if not questions_file.exists():
            logger.error("‚ùå Perguntas de teste n√£o encontradas")
            return []
        
        with open(questions_file, 'r', encoding='utf-8') as f:
            test_questions = json.load(f)
        
        # Inicializa sistema multi-cen√°rio
        system = MultiScenarioSystem()
        system.load_documents(documents)
        system.build_all_scenarios()
        
        # Status do sistema
        status = system.get_system_status()
        logger.info(f"üìä Status do sistema: {status}")
        
        # Executa compara√ß√£o para cada pergunta
        all_comparisons = []
        scenario_performance = {
            'scenario_a': {'total_time': 0, 'total_cost': 0, 'total_relevance': 0, 'success_count': 0},
            'scenario_b': {'total_time': 0, 'total_cost': 0, 'total_relevance': 0, 'success_count': 0},
            'scenario_c': {'total_time': 0, 'total_cost': 0, 'total_relevance': 0, 'success_count': 0}
        }
        
        for question in test_questions:
            logger.info(f"\n‚ùì Pergunta {question['id']}: {question['question']}")
            
            # Compara todos os cen√°rios
            comparison = system.compare_all_scenarios(question['question'])
            
            # Adiciona metadados da pergunta
            comparison['question_metadata'] = {
                'id': question['id'],
                'category': question['category'],
                'temporal_focus': question['temporal_focus'],
                'complexity': question['complexity'],
                'expected_concepts': question['expected_concepts']
            }
            
            all_comparisons.append(comparison)
            
            # Coleta m√©tricas de performance
            for scenario, result in comparison['results'].items():
                if 'error' not in result:
                    perf = scenario_performance[scenario]
                    perf['total_time'] += result.get('response_time', 0)
                    perf['total_cost'] += result.get('estimated_cost_usd', 0)
                    perf['total_relevance'] += result.get('relevance_score', 0)
                    perf['success_count'] += 1
                    
                    # Log resultado individual
                    logger.info(f"  {scenario}: {result.get('response_time', 0):.2f}s, "
                              f"${result.get('estimated_cost_usd', 0):.4f}, "
                              f"relev√¢ncia: {result.get('relevance_score', 0):.3f}")
                else:
                    logger.error(f"  {scenario}: ERRO - {result['error']}")
        
        # Calcula m√©tricas agregadas
        aggregated_metrics = {}
        for scenario, perf in scenario_performance.items():
            if perf['success_count'] > 0:
                aggregated_metrics[scenario] = {
                    'avg_response_time': perf['total_time'] / perf['success_count'],
                    'total_cost': perf['total_cost'],
                    'avg_relevance': perf['total_relevance'] / perf['success_count'],
                    'success_rate': perf['success_count'] / len(test_questions),
                    'successful_queries': perf['success_count']
                }
            else:
                aggregated_metrics[scenario] = {
                    'avg_response_time': 0,
                    'total_cost': 0,
                    'avg_relevance': 0,
                    'success_rate': 0,
                    'successful_queries': 0
                }
        
        # Salva resultados detalhados
        results_file = Path("data/evaluation/multi_scenario_comparison.json")
        detailed_results = {
            'execution_timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'system_status': status,
            'individual_comparisons': all_comparisons,
            'aggregated_metrics': aggregated_metrics,
            'test_summary': {
                'total_questions': len(test_questions),
                'documents_used': len(documents)
            }
        }
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(detailed_results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"‚úÖ Resultados salvos em {results_file}")
        return detailed_results
        
    except Exception as e:
        logger.error(f"‚ùå Erro na compara√ß√£o: {e}")
        return {}

def generate_comparative_report(results: Dict):
    """Gera relat√≥rio comparativo entre os cen√°rios"""
    logger.info("üìã Gerando relat√≥rio comparativo...")
    
    if not results or 'aggregated_metrics' not in results:
        logger.error("‚ùå Resultados insuficientes para relat√≥rio")
        return {}
    
    metrics = results['aggregated_metrics']
    
    # An√°lise comparativa
    report = {
        'execution_date': results.get('execution_timestamp', 'N/A'),
        'summary': {
            'total_questions_tested': results.get('test_summary', {}).get('total_questions', 0),
            'documents_analyzed': results.get('test_summary', {}).get('documents_used', 0)
        },
        'scenario_comparison': {},
        'recommendations': {
            'best_for_speed': None,
            'best_for_cost': None,
            'best_for_accuracy': None,
            'overall_recommendation': None,
            'detailed_analysis': []
        }
    }
    
    # An√°lise por cen√°rio
    for scenario, metric in metrics.items():
        scenario_name = {
            'scenario_a': 'Vector-Only RAG',
            'scenario_b': 'Hybrid RAG (Vector + Graph)', 
            'scenario_c': 'LLM-Only'
        }.get(scenario, scenario)
        
        report['scenario_comparison'][scenario_name] = {
            'average_response_time_sec': round(metric['avg_response_time'], 3),
            'total_cost_usd': round(metric['total_cost'], 4),
            'average_relevance_score': round(metric['avg_relevance'], 3),
            'success_rate': round(metric['success_rate'] * 100, 1),
            'successful_queries': metric['successful_queries']
        }
    
    # Determina melhores cen√°rios
    valid_scenarios = {k: v for k, v in metrics.items() if v['success_rate'] > 0}
    
    if valid_scenarios:
        # Melhor velocidade
        fastest = min(valid_scenarios, key=lambda x: valid_scenarios[x]['avg_response_time'])
        report['recommendations']['best_for_speed'] = fastest
        
        # Melhor custo
        cheapest = min(valid_scenarios, key=lambda x: valid_scenarios[x]['total_cost'])
        report['recommendations']['best_for_cost'] = cheapest
        
        # Melhor precis√£o
        most_accurate = max(valid_scenarios, key=lambda x: valid_scenarios[x]['avg_relevance'])
        report['recommendations']['best_for_accuracy'] = most_accurate
        
        # An√°lise detalhada
        analysis = []
        
        if fastest == 'scenario_c':
            analysis.append("LLM-Only oferece maior velocidade por n√£o ter overhead de recupera√ß√£o")
        elif fastest == 'scenario_a':
            analysis.append("Vector RAG oferece boa velocidade com informa√ß√µes contextualizadas")
        else:
            analysis.append("Hybrid RAG prioriza precis√£o sobre velocidade")
        
        if cheapest in ['scenario_a', 'scenario_b']:
            analysis.append("Cen√°rios RAG s√£o mais econ√¥micos por n√£o usar APIs externas")
        else:
            analysis.append("LLM-Only tem custos de API mas oferece respostas mais elaboradas")
        
        if most_accurate == 'scenario_b':
            analysis.append("Hybrid RAG com grafos oferece melhor racioc√≠nio temporal")
        elif most_accurate == 'scenario_a':
            analysis.append("Vector RAG oferece boa precis√£o com base documental")
        else:
            analysis.append("LLM-Only tem conhecimento geral robusto")
        
        report['recommendations']['detailed_analysis'] = analysis
        
        # Recomenda√ß√£o geral baseada em balance
        speed_rank = {v: k for k, v in enumerate(sorted(valid_scenarios, key=lambda x: valid_scenarios[x]['avg_response_time']))}
        cost_rank = {v: k for k, v in enumerate(sorted(valid_scenarios, key=lambda x: valid_scenarios[x]['total_cost']))}
        accuracy_rank = {v: k for k, v in enumerate(sorted(valid_scenarios, key=lambda x: valid_scenarios[x]['avg_relevance'], reverse=True))}
        
        # Score balanceado (menor √© melhor)
        balanced_scores = {}
        for scenario in valid_scenarios:
            score = speed_rank[scenario] + cost_rank[scenario] + accuracy_rank[scenario]
            balanced_scores[scenario] = score
        
        best_balanced = min(balanced_scores, key=balanced_scores.get)
        report['recommendations']['overall_recommendation'] = best_balanced
    
    # Salva relat√≥rio
    report_file = Path("data/evaluation/comparative_report.json")
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    logger.info(f"‚úÖ Relat√≥rio comparativo salvo em {report_file}")
    return report

def print_executive_summary(report: Dict):
    """Imprime resumo executivo dos resultados"""
    logger.info("\n" + "="*60)
    logger.info("üìä RESUMO EXECUTIVO - COMPARA√á√ÉO MULTI-CEN√ÅRIO")
    logger.info("="*60)
    
    if not report or 'scenario_comparison' not in report:
        logger.error("‚ùå Relat√≥rio inv√°lido")
        return
    
    # Informa√ß√µes gerais
    summary = report.get('summary', {})
    logger.info(f"üìã Perguntas testadas: {summary.get('total_questions_tested', 0)}")
    logger.info(f"üìÑ Documentos analisados: {summary.get('documents_analyzed', 0)}")
    logger.info(f"üìÖ Execu√ß√£o: {report.get('execution_date', 'N/A')}")
    
    # Performance por cen√°rio
    logger.info("\nüéØ PERFORMANCE POR CEN√ÅRIO:")
    for scenario, metrics in report['scenario_comparison'].items():
        logger.info(f"\n‚Ä¢ {scenario}:")
        logger.info(f"  ‚ö° Tempo m√©dio: {metrics['average_response_time_sec']}s")
        logger.info(f"  üí∞ Custo total: ${metrics['total_cost_usd']}")
        logger.info(f"  üéØ Relev√¢ncia m√©dia: {metrics['average_relevance_score']}")
        logger.info(f"  ‚úÖ Taxa de sucesso: {metrics['success_rate']}%")
    
    # Recomenda√ß√µes
    recommendations = report.get('recommendations', {})
    logger.info("\nüèÜ RECOMENDA√á√ïES:")
    
    if recommendations.get('best_for_speed'):
        logger.info(f"‚ö° Mais r√°pido: {recommendations['best_for_speed']}")
    
    if recommendations.get('best_for_cost'):
        logger.info(f"üí∞ Mais econ√¥mico: {recommendations['best_for_cost']}")
    
    if recommendations.get('best_for_accuracy'):
        logger.info(f"üéØ Mais preciso: {recommendations['best_for_accuracy']}")
    
    if recommendations.get('overall_recommendation'):
        logger.info(f"üåü Recomenda√ß√£o geral: {recommendations['overall_recommendation']}")
    
    analysis = recommendations.get('detailed_analysis', [])
    if analysis:
        logger.info("\nüìã AN√ÅLISE DETALHADA:")
        for point in analysis:
            logger.info(f"  ‚Ä¢ {point}")
    
    logger.info("\n" + "="*60)

def main():
    """Fun√ß√£o principal do MVP integrado"""
    logger.info("üöÄ EXECUTANDO MVP INTEGRADO - COMPARA√á√ÉO MULTI-CEN√ÅRIO")
    logger.info("="*70)
    
    # Passo 1: Verificar ambiente
    if not check_environment():
        logger.error("‚ùå Ambiente n√£o configurado adequadamente")
        return False
    
    # Passo 2: Validar configura√ß√£o base
    sys.path.append('src')
    try:
        from config import Config
        if not Config.validate_setup():
            logger.error("‚ùå Configura√ß√£o base inv√°lida")
            return False
    except ImportError:
        logger.warning("‚ö†Ô∏è Config n√£o encontrado, continuando...")
    
    # Passo 3: Criar dados de exemplo enriquecidos
    enhanced_docs = create_enhanced_sample_data()
    temporal_questions = create_temporal_test_questions()
    
    # Passo 4: Executar compara√ß√£o multi-cen√°rio
    comparison_results = run_multi_scenario_comparison()
    
    if not comparison_results:
        logger.error("‚ùå Falha na compara√ß√£o multi-cen√°rio")
        return False
    
    # Passo 5: Gerar relat√≥rio comparativo
    comparative_report = generate_comparative_report(comparison_results)
    
    # Passo 6: Exibir resumo executivo
    print_executive_summary(comparative_report)
    
    # Passo 7: Pr√≥ximos passos
    logger.info("\nüìã PR√ìXIMOS PASSOS:")
    logger.info("   1. Execute: streamlit run app/multi_scenario_dashboard.py")
    logger.info("   2. Analise os resultados detalhados nos arquivos JSON")
    logger.info("   3. Teste com seus pr√≥prios documentos e perguntas")
    logger.info("   4. Escolha o cen√°rio mais adequado para produ√ß√£o")
    
    logger.info(f"\n‚úÖ MVP INTEGRADO CONCLU√çDO COM SUCESSO!")
    return True

if __name__ == "__main__":
    success = main()
    if not success:
        sys.exit(1)
    