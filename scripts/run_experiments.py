#!/usr/bin/env python3
"""
Script principal para executar todos os experimentos de compara√ß√£o LLM
Otimizado para Mac M1 com Metal Performance Shaders
"""

import logging
import sys
import json
from pathlib import Path
from typing import Dict, List, Any
import pandas as pd
import torch
from datetime import datetime

# Adiciona src ao path
sys.path.append(str(Path(__file__).parent.parent))

from src.config import Config
from src.data_processing.ingestion import JsonDocumentIngestionPipeline
from src.evaluation.metrics import EvaluationMetrics
from src.evaluation.benchmark import BenchmarkGenerator
from src.evaluation.cost_analysis import CostAnalyzer

# Configura√ß√£o de logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/experiments.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

class ExperimentRunner:
    def __init__(self):
        self.config = Config()
        self.config.setup_directories()
        self.results = {}
        
        # Verifica disponibilidade de acelera√ß√£o
        self._check_hardware()
    
    def _check_hardware(self):
        """Verifica e loga informa√ß√µes de hardware"""
        if torch.backends.mps.is_available():
            logger.info("‚úÖ Metal Performance Shaders (MPS) dispon√≠vel - M1/M2 Mac")
            self.device = "mps"
        elif torch.cuda.is_available():
            logger.info(f"‚úÖ CUDA dispon√≠vel - {torch.cuda.device_count()} GPU(s)")
            self.device = "cuda"
        else:
            logger.info("‚ö†Ô∏è  Usando CPU - considere usar Colab para acelera√ß√£o")
            self.device = "cpu"
    
    def step_1_data_ingestion(self):
        """Passo 1: Ingest√£o e processamento de dados"""
        logger.info("=== PASSO 1: INGEST√ÉO DE DADOS ===")
        
        data_dir = self.config.DATA_DIR / "raw"
        processed_dir = self.config.DATA_DIR / "processed"
        embeddings_dir = self.config.DATA_DIR / "embeddings"
        
        if not data_dir.exists() or not list(data_dir.iterdir()):
            logger.warning(f"Diret√≥rio {data_dir} vazio. Criando dados de exemplo...")
            self._create_sample_data(data_dir)
        
        # Pipeline de ingest√£o
        pipeline = JsonDocumentIngestionPipeline(self.config)
        
        try:
            pipeline.run(data_dir, embeddings_dir)
            self.results['data_ingestion'] = {
                'status': 'success',
                'num_chunks': len(pipeline.chunks),
                'embedding_dim': pipeline.embeddings.shape[1] if pipeline.embeddings is not None else 0,
                'device_used': pipeline.embedding_model.device
            }
            logger.info("‚úÖ Ingest√£o de dados conclu√≠da")
        except Exception as e:
            logger.error(f"‚ùå Erro na ingest√£o: {e}")
            self.results['data_ingestion'] = {'status': 'failed', 'error': str(e)}
            return False
        
        return True
    
    def step_2_create_golden_dataset(self):
        """Passo 2: Cria√ß√£o do dataset golden"""
        logger.info("=== PASSO 2: DATASET GOLDEN ===")
        
        golden_dir = self.config.DATA_DIR / "golden_set"
        golden_file = golden_dir / "golden_dataset.json"
        
        if golden_file.exists():
            logger.info("Dataset golden j√° existe, carregando...")
            with open(golden_file, 'r', encoding='utf-8') as f:
                golden_data = json.load(f)
        else:
            logger.info("Criando novo dataset golden...")
            
            # Carrega documentos processados
            embeddings_dir = self.config.DATA_DIR / "embeddings"
            docs_file = embeddings_dir / "documents.json"
            
            if not docs_file.exists():
                logger.error("Documentos n√£o encontrados. Execute primeiro a ingest√£o.")
                return False
            
            docs_df = pd.read_json(docs_file, orient='records')
            documents = docs_df['content'].head(10).tolist()  # Primeiros 10 docs
            
            # Gera dataset golden
            generator = BenchmarkGenerator()
            golden_data = generator.create_golden_dataset(
                documents=documents,
                seed_questions=[
                    "Como monitorar ru√≠do em obras urbanas?",
                    "Quais s√£o os padr√µes de seguran√ßa para canteiros?",
                    "Como calcular impacto ambiental de constru√ß√µes?",
                    "Que equipamentos s√£o necess√°rios para obras noturnas?",
                    "Como implementar controle de qualidade em constru√ß√£o?"
                ],
                output_path=golden_file
            )
        
        self.results['golden_dataset'] = {
            'status': 'success',
            'num_synthetic_qa': len(golden_data.get('synthetic_qa', [])),
            'num_seed_questions': len(golden_data.get('seed_questions', [])),
            'num_adversarial': len(golden_data.get('adversarial_questions', []))
        }
        
        logger.info("‚úÖ Dataset golden preparado")
        return True
    
    def step_3_test_rag_simple(self):
        """Passo 3: Teste do RAG simples"""
        logger.info("=== PASSO 3: RAG SIMPLES ===")
        
        try:
            from src.architectures.rag_simple import SimpleRAGSystem
            
            # Inicializa sistema RAG
            rag_system = SimpleRAGSystem(self.config, model_name="phi-2")
            
            # Carrega √≠ndice
            embeddings_dir = self.config.DATA_DIR / "embeddings"
            rag_system.load_index(embeddings_dir)
            
            # Testa com algumas perguntas
            test_questions = [
                "Como monitorar ru√≠do em obras?",
                "Quais equipamentos de seguran√ßa s√£o obrigat√≥rios?",
                "Como controlar impacto ambiental na constru√ß√£o?"
            ]
            
            responses = []
            for question in test_questions:
                result = rag_system.query(question)
                responses.append({
                    'question': question,
                    'answer': result['answer'],
                    'num_docs_retrieved': result['metadata']['num_retrieved']
                })
            
            # Salva resultados de teste
            test_results_file = self.config.DATA_DIR / "evaluation" / "rag_simple_test.json"
            with open(test_results_file, 'w', encoding='utf-8') as f:
                json.dump(responses, f, ensure_ascii=False, indent=2)
            
            self.results['rag_simple'] = {
                'status': 'success',
                'test_questions': len(test_questions),
                'avg_retrieved_docs': sum(r['num_docs_retrieved'] for r in responses) / len(responses),
                'model_used': rag_system.model_config.name
            }
            
            logger.info("‚úÖ RAG simples testado com sucesso")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Erro no teste RAG: {e}")
            self.results['rag_simple'] = {'status': 'failed', 'error': str(e)}
            return False
    
    def step_4_evaluation_metrics(self):
        """Passo 4: Avalia√ß√£o com m√©tricas"""
        logger.info("=== PASSO 4: AVALIA√á√ÉO M√âTRICAS ===")
        
        try:
            # Carrega dataset golden
            golden_file = self.config.DATA_DIR / "golden_set" / "golden_dataset.json"
            with open(golden_file, 'r', encoding='utf-8') as f:
                golden_data = json.load(f)
            
            # Carrega resultados do RAG
            test_results_file = self.config.DATA_DIR / "evaluation" / "rag_simple_test.json"
            with open(test_results_file, 'r', encoding='utf-8') as f:
                rag_results = json.load(f)
            
            # Prepara dados para avalia√ß√£o
            predictions = [r['answer'] for r in rag_results]
            references = [r['question'] for r in rag_results]  # Placeholder
            
            # Calcula m√©tricas
            evaluator = EvaluationMetrics()
            metrics = evaluator.evaluate_response_quality(predictions, references)
            
            # Salva m√©tricas
            metrics_file = self.config.DATA_DIR / "evaluation" / "metrics_results.json"
            with open(metrics_file, 'w', encoding='utf-8') as f:
                json.dump(metrics, f, ensure_ascii=False, indent=2)
            
            self.results['evaluation'] = {
                'status': 'success',
                'metrics': metrics
            }
            
            logger.info(f"‚úÖ Avalia√ß√£o conclu√≠da: ROUGE-L: {metrics.get('rougeL', 0):.3f}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Erro na avalia√ß√£o: {e}")
            self.results['evaluation'] = {'status': 'failed', 'error': str(e)}
            return False
    
    def step_5_cost_analysis(self):
        """Passo 5: An√°lise de custos"""
        logger.info("=== PASSO 5: AN√ÅLISE DE CUSTOS ===")
        
        try:
            analyzer = CostAnalyzer()
            
            # Cen√°rios de teste
            scenarios = [
                {'name': 'Piloto', 'queries_per_day': 100},
                {'name': 'Produ√ß√£o Pequena', 'queries_per_day': 1000},
                {'name': 'Produ√ß√£o M√©dia', 'queries_per_day': 5000}
            ]
            
            cost_report = analyzer.compare_architectures(scenarios)
            
            # Salva relat√≥rio
            cost_file = self.config.DATA_DIR / "evaluation" / "cost_analysis.csv"
            cost_report.to_csv(cost_file, index=False)
            
            self.results['cost_analysis'] = {
                'status': 'success',
                'scenarios_analyzed': len(scenarios),
                'architectures_compared': len(cost_report['architecture'].unique())
            }
            
            logger.info("‚úÖ An√°lise de custos conclu√≠da")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Erro na an√°lise de custos: {e}")
            self.results['cost_analysis'] = {'status': 'failed', 'error': str(e)}
            return False
    
    def _create_sample_data(self, data_dir: Path):
        """Cria dados de exemplo se n√£o existirem"""
        data_dir.mkdir(parents=True, exist_ok=True)
        
        sample_docs = [
            {
                "content": "O monitoramento de ru√≠do em obras urbanas deve seguir as diretrizes da NBR 10151. Os n√≠veis de ru√≠do n√£o podem exceder 70 dB durante o dia e 60 dB durante a noite em √°reas residenciais. √â obrigat√≥rio o uso de medidores calibrados e registros hor√°rios.",
                "title": "Normas de Ru√≠do em Obras",
                "category": "regulamentacao"
            },
            {
                "content": "Equipamentos de prote√ß√£o individual (EPIs) obrigat√≥rios em canteiros incluem: capacete, √≥culos de prote√ß√£o, luvas, cal√ßados de seguran√ßa, cintos de seguran√ßa para trabalho em altura. A empresa deve fornecer gratuitamente e treinar os trabalhadores.",
                "title": "EPIs Obrigat√≥rios",
                "category": "seguranca"
            },
            {
                "content": "O controle de impacto ambiental em constru√ß√µes requer licenciamento pr√©vio, plano de gerenciamento de res√≠duos, controle de eros√£o e prote√ß√£o de recursos h√≠dricos. Obras acima de 3000m¬≤ necessitam de estudo de impacto ambiental.",
                "title": "Impacto Ambiental",
                "category": "meio_ambiente"
            },
            {
                "content": "Obras noturnas requerem autoriza√ß√£o especial da prefeitura, ilumina√ß√£o adequada (m√≠nimo 200 lux), sinaliza√ß√£o refor√ßada e equipes especializadas. O hor√°rio permitido varia entre 22h e 6h, dependendo da localiza√ß√£o.",
                "title": "Obras Noturnas",
                "category": "regulamentacao"
            },
            {
                "content": "Controle de qualidade em constru√ß√£o envolve inspe√ß√µes regulares, testes de materiais, documenta√ß√£o de procedimentos e auditorias internas. Concreto deve ser testado a cada 50m¬≥ ou a cada dia de concretagem.",
                "title": "Controle de Qualidade",
                "category": "qualidade"
            }
        ]
        
        for i, doc in enumerate(sample_docs):
            doc_file = data_dir / f"documento_{i+1:02d}.json"
            with open(doc_file, 'w', encoding='utf-8') as f:
                json.dump(doc, f, ensure_ascii=False, indent=2)
        
        logger.info(f"‚úÖ Criados {len(sample_docs)} documentos de exemplo em {data_dir}")
    
    def generate_final_report(self):
        """Gera relat√≥rio final dos experimentos"""
        logger.info("=== RELAT√ìRIO FINAL ===")
        
        report = {
            'experiment_date': datetime.now().isoformat(),
            'hardware_used': self.device,
            'config': {
                'chunk_size': self.config.RAG.chunk_size,
                'embedding_model': self.config.RAG.embedding_model,
                'top_k': self.config.RAG.top_k
            },
            'results': self.results
        }
        
        # Salva relat√≥rio completo
        report_file = self.config.DATA_DIR / "evaluation" / "final_report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        # Log sum√°rio
        logger.info("üìä SUM√ÅRIO DOS RESULTADOS:")
        for step, result in self.results.items():
            status = result.get('status', 'unknown')
            emoji = "‚úÖ" if status == "success" else "‚ùå"
            logger.info(f"   {emoji} {step}: {status}")
        
        logger.info(f"üìÑ Relat√≥rio completo salvo em: {report_file}")
        
        return report
    
    def run_all_experiments(self):
        """Executa todos os experimentos em sequ√™ncia"""
        logger.info("üöÄ INICIANDO EXPERIMENTOS LLM COMPARA√á√ÉO")
        logger.info(f"üì± Hardware: {self.device}")
        
        steps = [
            ("Ingest√£o de Dados", self.step_1_data_ingestion),
            ("Dataset Golden", self.step_2_create_golden_dataset),
            ("RAG Simples", self.step_3_test_rag_simple),
            ("Avalia√ß√£o M√©tricas", self.step_4_evaluation_metrics),
            ("An√°lise de Custos", self.step_5_cost_analysis)
        ]
        
        for step_name, step_func in steps:
            logger.info(f"\n{'='*50}")
            logger.info(f"Executando: {step_name}")
            logger.info(f"{'='*50}")
            
            try:
                success = step_func()
                if not success:
                    logger.error(f"‚ùå Falha em: {step_name}")
                    break
            except Exception as e:
                logger.error(f"‚ùå Erro em {step_name}: {e}")
                break
        
        # Gera relat√≥rio final
        self.generate_final_report()
        
        logger.info("\nüéâ EXPERIMENTOS CONCLU√çDOS!")
        logger.info("üìä Execute 'streamlit run app/streamlit_dashboard.py' para ver os resultados")


def main():
    """Fun√ß√£o principal"""
    runner = ExperimentRunner()
    runner.run_all_experiments()


if __name__ == "__main__":
    main()
    